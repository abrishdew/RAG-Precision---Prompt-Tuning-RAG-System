{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_eval(prompt):\n",
    "    # Simulating different types of responses\n",
    "    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']\n",
    "    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}\n",
    "\n",
    "    # Perform multiple random trials\n",
    "    trials = 100\n",
    "    total_score = 0\n",
    "    for _ in range(trials):\n",
    "        response = random.choice(response_types)\n",
    "        total_score += scores[response]\n",
    "\n",
    "    # Average score represents the evaluation\n",
    "    return total_score / trials\n",
    "\n",
    "def elo_eval(prompt, base_rating=1500):\n",
    "    # Simulate the outcome of the prompt against standard criteria\n",
    "    # Here, we randomly decide if the prompt 'wins', 'loses', or 'draws'\n",
    "    outcomes = ['win', 'loss', 'draw']\n",
    "    outcome = random.choice(outcomes)\n",
    "\n",
    "    # Elo rating formula parameters\n",
    "    K = 30  # Maximum change in rating\n",
    "    R_base = 10 ** (base_rating / 400)\n",
    "    R_opponent = 10 ** (1600 / 400)  # Assuming a fixed opponent rating\n",
    "    expected_score = R_base / (R_base + R_opponent)\n",
    "\n",
    "    # Calculate the new rating based on the outcome\n",
    "    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
    "    new_rating = base_rating + K * (actual_score - expected_score)\n",
    "\n",
    "    return new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):\n",
    "    \"\"\"\n",
    "    Update Elo ratings for a list of prompts based on simulated outcomes.\n",
    "\n",
    "    Parameters:\n",
    "    prompts (list): List of prompts to be evaluated.\n",
    "    elo_ratings (dict): Current Elo ratings for each prompt.\n",
    "    K (int): Maximum change in rating.\n",
    "    opponent_rating (int): Fixed rating of the opponent for simulation.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated Elo ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    for prompt in prompts:\n",
    "        # Simulate an outcome against the standard criteria or another prompt\n",
    "        outcome = random.choice(['win', 'loss', 'draw'])\n",
    "\n",
    "        # Calculate the new rating based on the outcome\n",
    "        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
    "        R_base = 10 ** (elo_ratings[prompt] / 400)\n",
    "        R_opponent = 10 ** (opponent_rating / 400)\n",
    "        expected_score = R_base / (R_base + R_opponent)\n",
    "        elo_ratings[prompt] += K * (actual_score - expected_score)\n",
    "\n",
    "    return elo_ratings\n",
    "\n",
    "# Example usage\n",
    "prompts = [\"Who founded OpenAI?\", \n",
    "                \"What was the initial goal of OpenAI?\",\n",
    "                \"What did OpenAI release in 2016?\", \n",
    "                \"What project did OpenAI showcase in 2018?\",\n",
    "                \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
    "\n",
    "# Conduct multiple rounds of evaluation\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = elo_ratings_func(prompts, elo_ratings)\n",
    "\n",
    "# Sort prompts by their final Elo ratings\n",
    "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "# Print the ranked prompts\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(main_prompt, test_cases):\n",
    "    evaluations = {}\n",
    "\n",
    "    # Evaluate the main prompt using Monte Carlo and Elo methods\n",
    "    evaluations['main_prompt'] = {\n",
    "        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),\n",
    "        'Elo Rating Evaluation': elo_eval(main_prompt)\n",
    "    }\n",
    "\n",
    "    # Evaluate each test case\n",
    "    for idx, test_case in enumerate(test_cases):\n",
    "        evaluations[f'test_case_{idx+1}'] = {\n",
    "            'Monte Carlo Evaluation': monte_carlo_eval(test_case),\n",
    "            'Elo Rating Evaluation': elo_eval(test_case)\n",
    "        }\n",
    "\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = \"what is this week's challenged all about?\"\n",
    "test_cases = [\"Who is the company involved with ths week's challenge?\", \n",
    "                \"What was the initial idea for the project?\",\n",
    "                \"What services are provided by the company?\", \n",
    "                \"What is the competency mapping for this week?\",\n",
    "                \"How are the points allocated for each work?\"\n",
    "                ]\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
